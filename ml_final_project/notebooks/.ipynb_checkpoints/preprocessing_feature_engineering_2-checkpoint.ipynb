{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['application_test.csv', '.DS_Store', 'POS_CASH_balance.csv', 'credit_card_balance.csv', 'installments_payments.csv', 'application_train.csv', 'bureau.csv', 'previous_application.csv', 'preprocessing-and-feature-engineering-one', 'bureau_balance.csv']\n"
     ]
    }
   ],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Memory management\n",
    "import gc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4f7a6ced7bacef6452d925430941434db5f99c9"
   },
   "source": [
    "# Import Data From Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f286d677e1a561e65205d8285540dcc778432926"
   },
   "source": [
    "## Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9c83de58d7e9323358bffa7b6a6436fc191760b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 317)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>...</th>\n",
       "      <th>client_bureau_balance_STATUS_C_count_mean</th>\n",
       "      <th>client_bureau_balance_STATUS_C_count_min</th>\n",
       "      <th>client_bureau_balance_STATUS_C_count_norm_max</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_mean</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_max</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_min</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_norm_mean</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_norm_max</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_norm_min</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>-3648.0</td>\n",
       "      <td>-2120</td>\n",
       "      <td>...</td>\n",
       "      <td>2.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161932</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>-1186.0</td>\n",
       "      <td>-291</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>-4260.0</td>\n",
       "      <td>-2531</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>-9833.0</td>\n",
       "      <td>-2437</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>-4311.0</td>\n",
       "      <td>-3458</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0      100002             0          202500.0    406597.5      24700.5   \n",
       "1      100003             0          270000.0   1293502.5      35698.5   \n",
       "2      100004             0           67500.0    135000.0       6750.0   \n",
       "3      100006             0          135000.0    312682.5      29686.5   \n",
       "4      100007             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  \\\n",
       "0                    0.018801        9461           -637            -3648.0   \n",
       "1                    0.003541       16765          -1188            -1186.0   \n",
       "2                    0.010032       19046           -225            -4260.0   \n",
       "3                    0.008019       19005          -3039            -9833.0   \n",
       "4                    0.028663       19932          -3038            -4311.0   \n",
       "\n",
       "   DAYS_ID_PUBLISH   ...    client_bureau_balance_STATUS_C_count_mean  \\\n",
       "0            -2120   ...                                        2.875   \n",
       "1             -291   ...                                          NaN   \n",
       "2            -2531   ...                                          NaN   \n",
       "3            -2437   ...                                          NaN   \n",
       "4            -3458   ...                                          NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_C_count_min  \\\n",
       "0                                       0.0   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_C_count_norm_max  \\\n",
       "0                                         0.8125   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_mean  \\\n",
       "0                                      1.875   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_max  \\\n",
       "0                                       3.0   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_min  \\\n",
       "0                                       0.0   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_norm_mean  \\\n",
       "0                                        0.161932   \n",
       "1                                             NaN   \n",
       "2                                             NaN   \n",
       "3                                             NaN   \n",
       "4                                             NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_norm_max  \\\n",
       "0                                            0.5   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_norm_min  TARGET  \n",
       "0                                            0.0       1  \n",
       "1                                            NaN       0  \n",
       "2                                            NaN       0  \n",
       "3                                            NaN       0  \n",
       "4                                            NaN       0  \n",
       "\n",
       "[5 rows x 317 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "train = pd.read_csv('../input/preprocessing-and-feature-engineering-one/train_corrs_removed.csv')\n",
    "print('Training data shape: ', train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12cedcbb7c51971aa8a5e63d76fe660df0ae2594"
   },
   "source": [
    "## Import Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "6ea002847fcbe63c0a3df380addc58a962081ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (48744, 316)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>...</th>\n",
       "      <th>client_bureau_balance_STATUS_5_count_min</th>\n",
       "      <th>client_bureau_balance_STATUS_C_count_mean</th>\n",
       "      <th>client_bureau_balance_STATUS_C_count_min</th>\n",
       "      <th>client_bureau_balance_STATUS_C_count_norm_max</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_mean</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_max</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_min</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_norm_mean</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_norm_max</th>\n",
       "      <th>client_bureau_balance_STATUS_X_count_norm_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>568800.0</td>\n",
       "      <td>20560.5</td>\n",
       "      <td>0.018850</td>\n",
       "      <td>-19241</td>\n",
       "      <td>-2329.0</td>\n",
       "      <td>-5170.0</td>\n",
       "      <td>-812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214590</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>222768.0</td>\n",
       "      <td>17370.0</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>-18064</td>\n",
       "      <td>-4469.0</td>\n",
       "      <td>-9118.0</td>\n",
       "      <td>-1623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136752</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>663264.0</td>\n",
       "      <td>69777.0</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>-20038</td>\n",
       "      <td>-4458.0</td>\n",
       "      <td>-2175.0</td>\n",
       "      <td>-3503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>2</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>49018.5</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>-13976</td>\n",
       "      <td>-1866.0</td>\n",
       "      <td>-2000.0</td>\n",
       "      <td>-4208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>11.083333</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>1</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>32067.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-13040</td>\n",
       "      <td>-2191.0</td>\n",
       "      <td>-4000.0</td>\n",
       "      <td>-4262</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0      100001             0          135000.0    568800.0      20560.5   \n",
       "1      100005             0           99000.0    222768.0      17370.0   \n",
       "2      100013             0          202500.0    663264.0      69777.0   \n",
       "3      100028             2          315000.0   1575000.0      49018.5   \n",
       "4      100038             1          180000.0    625500.0      32067.0   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  \\\n",
       "0                    0.018850      -19241        -2329.0            -5170.0   \n",
       "1                    0.035792      -18064        -4469.0            -9118.0   \n",
       "2                    0.019101      -20038        -4458.0            -2175.0   \n",
       "3                    0.026392      -13976        -1866.0            -2000.0   \n",
       "4                    0.010032      -13040        -2191.0            -4000.0   \n",
       "\n",
       "   DAYS_ID_PUBLISH                      ...                        \\\n",
       "0             -812                      ...                         \n",
       "1            -1623                      ...                         \n",
       "2            -3503                      ...                         \n",
       "3            -4208                      ...                         \n",
       "4            -4262                      ...                         \n",
       "\n",
       "   client_bureau_balance_STATUS_5_count_min  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_C_count_mean  \\\n",
       "0                                  15.714286   \n",
       "1                                   1.666667   \n",
       "2                                  25.750000   \n",
       "3                                  21.916667   \n",
       "4                                        NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_C_count_min  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_C_count_norm_max  \\\n",
       "0                                       0.966667   \n",
       "1                                       0.384615   \n",
       "2                                       0.666667   \n",
       "3                                       0.885714   \n",
       "4                                            NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_mean  \\\n",
       "0                                   4.285714   \n",
       "1                                   0.666667   \n",
       "2                                  10.250000   \n",
       "3                                  11.083333   \n",
       "4                                        NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_max  \\\n",
       "0                                       9.0   \n",
       "1                                       1.0   \n",
       "2                                      40.0   \n",
       "3                                      60.0   \n",
       "4                                       NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_min  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_norm_mean  \\\n",
       "0                                        0.214590   \n",
       "1                                        0.136752   \n",
       "2                                        0.254545   \n",
       "3                                        0.260434   \n",
       "4                                             NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_norm_max  \\\n",
       "0                                       0.500000   \n",
       "1                                       0.333333   \n",
       "2                                       1.000000   \n",
       "3                                       1.000000   \n",
       "4                                            NaN   \n",
       "\n",
       "   client_bureau_balance_STATUS_X_count_norm_min  \n",
       "0                                            0.0  \n",
       "1                                            0.0  \n",
       "2                                            0.0  \n",
       "3                                            0.0  \n",
       "4                                            NaN  \n",
       "\n",
       "[5 rows x 316 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing data\n",
    "test = pd.read_csv('../input/preprocessing-and-feature-engineering-one/test_corrs_removed.csv')\n",
    "print('Training data shape: ', test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc1266bc40ddc394dbedd967c01939e00595c20a"
   },
   "source": [
    "## Function to Aggregate Numeric Data\n",
    "This groups data by the group_var and calculates mean, max, min, and sum. It will only be applied to numeric data by default in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "5b0c07c602ca87c3724aa6e8cabf8916a86e03d5"
   },
   "outputs": [],
   "source": [
    "def agg_numeric(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Groups and aggregates the numeric values in a child dataframe\n",
    "    by the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the child dataframe to calculate the statistics on\n",
    "        parent_var (string): \n",
    "            the parent variable used for grouping and aggregating\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated by the `parent_var` for \n",
    "            all numeric columns. Each observation of the parent variable will have \n",
    "            one row in the dataframe with the parent variable as the index. \n",
    "            The columns are also renamed using the `df_name`. Columns with all duplicate\n",
    "            values are removed. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != parent_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    # Only want the numeric variables\n",
    "    parent_ids = df[parent_var].copy()\n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[parent_var] = parent_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = []\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        if var != parent_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    agg.columns = columns\n",
    "    \n",
    "    # Remove the columns with all redundant values\n",
    "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29d3b477e08d1c67b78a1eca83d4db5162576e0a"
   },
   "source": [
    "## Function to Calculate Categorical Count\n",
    "This calculates the occurrences (counts) of each category in a categorical variable for each client. It also calculates the normed count, which is the count for a category divided by the total counts for all categories in a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3dc87e670deb50c5b8a906391abaae9a9dc31cc8"
   },
   "outputs": [],
   "source": [
    "def agg_categorical(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregates the categorical features in a child dataframe\n",
    "    for each observation of the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    parent_var : string\n",
    "        The variable by which to group and aggregate the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with aggregated statistics for each observation of the parent_var\n",
    "        The columns are also renamed and columns with duplicate values are removed.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('category'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[parent_var] = df[parent_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    # Remove duplicate columns by values\n",
    "    _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
    "    categorical = categorical.iloc[:, idx]\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "218ba8c1634081f158cc39b48124d07ce5cecfa0"
   },
   "source": [
    "## Function to Aggregate Stats at the Client Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2acf90ab5b22eb2ae39c9c2e1671d9a3eb04eacf"
   },
   "outputs": [],
   "source": [
    "def aggregate_client(df, group_vars, df_names):\n",
    "    \"\"\"Aggregate a dataframe with data at the loan level \n",
    "    at the client level\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): data at the loan level\n",
    "        group_vars (list of two strings): grouping variables for the loan \n",
    "        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n",
    "        names (list of two strings): names to call the resulting columns\n",
    "        (example ['cash', 'client'])\n",
    "        \n",
    "    Returns:\n",
    "        df_client (dataframe): aggregated numeric stats at the client level. \n",
    "        Each client will have a single row with all the numeric data aggregated\n",
    "    \"\"\"\n",
    "    \n",
    "    # Aggregate the numeric columns\n",
    "    df_agg = agg_numeric(df, parent_var = group_vars[0], df_name = df_names[0])\n",
    "    \n",
    "    # If there are categorical variables\n",
    "    if any(df.dtypes == 'category'):\n",
    "    \n",
    "        # Count the categorical columns\n",
    "        df_counts = agg_categorical(df, parent_var = group_vars[0], df_name = df_names[0])\n",
    "\n",
    "        # Merge the numeric and categorical\n",
    "        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n",
    "\n",
    "        gc.enable()\n",
    "        del df_agg, df_counts\n",
    "        gc.collect()\n",
    "\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
    "\n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
    "\n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
    "\n",
    "        \n",
    "    # No categorical variables\n",
    "    else:\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
    "        \n",
    "        gc.enable()\n",
    "        del df_agg\n",
    "        gc.collect()\n",
    "        \n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
    "        \n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
    "        \n",
    "    # Memory management\n",
    "    gc.enable()\n",
    "    del df, df_by_loan\n",
    "    gc.collect()\n",
    "\n",
    "    return df_by_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "898af47d49985f94e2ca4340a1b8ee21130e26ac"
   },
   "source": [
    "## Function for KDE Plots of Variable\n",
    "This plots the distribution of variable colored by the value of TARGET (either 1 for did not repay the loan or 0 for did repay the loan). We can use this function to visually examine any new variables we create. This also calculates the correlation cofficient of the variable with the target which can be used as an approximation of whether or not the created variable will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "88c7136315296bbe60c07c0417635a8c77c19868"
   },
   "outputs": [],
   "source": [
    "# Plots the disribution of a variable colored by value of the target\n",
    "def kde_target(var_name, df):\n",
    "    \n",
    "    # Calculate the correlation coefficient between the new variable and the target\n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "    \n",
    "    # Calculate medians for repaid vs not repaid\n",
    "    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n",
    "    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n",
    "    \n",
    "    plt.figure(figsize = (12, 6))\n",
    "    \n",
    "    # Plot the distribution for target == 0 and target == 1\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n",
    "    \n",
    "    # label the plot\n",
    "    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n",
    "    plt.legend();\n",
    "    \n",
    "    # print out the correlation\n",
    "    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n",
    "    # Print out average values\n",
    "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n",
    "    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b587a2ed1af10450cb90fc31c916ddca7762fa8a"
   },
   "source": [
    "## Function to Convert Data Types\n",
    "This will help reduce memory usage by using more efficient types for the variables. For example category is often a better type than object (unless the number of unique categories is close to the number of rows in the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "bb4f8cf6cfa9e0cba88718832993d9f4b9454e0c"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3749467140b877d9d2d2b45f6c921782faf922b6"
   },
   "source": [
    "## Function to Calculate Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "eb640a9b6fe20a5f9f6a6d0f5d724baffb3d1239"
   },
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df, print_info = False):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        if print_info:\n",
    "            # Print some summary information\n",
    "            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "                  \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d1bbbf70e67bdd2e632ec9740f094327e1472fc5"
   },
   "outputs": [],
   "source": [
    "def remove_missing_columns(train, test, threshold = 80):\n",
    "    # Calculate missing stats for train and test (remember to calculate a percent!)\n",
    "    train_miss = pd.DataFrame(train.isnull().sum())\n",
    "    train_miss['percent'] = 100 * train_miss[0] / len(train)\n",
    "    \n",
    "    test_miss = pd.DataFrame(test.isnull().sum())\n",
    "    test_miss['percent'] = 100 * test_miss[0] / len(test)\n",
    "    \n",
    "    # list of missing columns for train and test\n",
    "    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n",
    "    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n",
    "    \n",
    "    # Combine the two lists together\n",
    "    missing_columns = list(set(missing_train_columns + missing_test_columns))\n",
    "    \n",
    "    # Print information\n",
    "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n",
    "    \n",
    "    # Drop the missing columns and return\n",
    "    train = train.drop(columns = missing_columns)\n",
    "    test = test.drop(columns = missing_columns)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88c933feb1c392790ce82aca2da7939f639976aa"
   },
   "source": [
    "# Feature Engineering: one dataframe at a time for final four dataframes\n",
    "Dataframes: previous_application, POS_CASH_BALANCE (cash), credit_card_balance (credit), installments_payment (installments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4df3f640a65785bab84fa3c0f0f493da88a6361a"
   },
   "source": [
    "## Feature Engineering: previous_application dataframe\n",
    "previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6c518228271c11319f01246a1ec50e657485a77b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/home-credit-default-risk/previous_application.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-76b862a21dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprevious\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/home-credit-default-risk/previous_application.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprevious\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprevious\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/home-credit-default-risk/previous_application.csv' does not exist"
     ]
    }
   ],
   "source": [
    "previous = pd.read_csv('../input/previous_application.csv')\n",
    "previous = convert_types(previous, print_info=True)\n",
    "previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7376c3d5f21119558374ec5f975eb31ba093ea8"
   },
   "outputs": [],
   "source": [
    "# Calculate aggregate statistics for each numeric column\n",
    "previous_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\n",
    "print('Previous aggregation shape: ', previous_agg.shape)\n",
    "previous_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d932d84ca5473a8deabc25b3fee32dc92675eba5"
   },
   "outputs": [],
   "source": [
    "# Calculate value counts for each categorical column\n",
    "previous_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\n",
    "print('Previous counts shape: ', previous_counts.shape)\n",
    "previous_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c0dbbbda34ca4db6bb835b47e24a0c95bcff8bd"
   },
   "outputs": [],
   "source": [
    "## Merge into training and testing data\n",
    "train = convert_types(train)\n",
    "test = convert_types(test)\n",
    "\n",
    "# Merge in the previous information\n",
    "train = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "train = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "test = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "test = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Remove variables to free memory\n",
    "gc.enable()\n",
    "del previous, previous_agg, previous_counts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d867ac6845e8d69cfd88510a121dfade9e7b2b9"
   },
   "outputs": [],
   "source": [
    "train, test = remove_missing_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4de12a01afd5ee13fe5585fa8772cf070604ddb0"
   },
   "source": [
    "## Feature Engineering: POS_CASH_BALANCE (cash) dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54640cceaedbcd9f2bf337c4a9eabd6ab805c0f9"
   },
   "outputs": [],
   "source": [
    "cash = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\n",
    "cash = convert_types(cash, print_info=True)\n",
    "cash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c2f41fef20b4127b52d9c9cd05049f1896ec9de"
   },
   "outputs": [],
   "source": [
    "cash_by_client = aggregate_client(cash, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\n",
    "cash_by_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd619ff59d7a07998db17cb9c1be9a67755bb8f8"
   },
   "outputs": [],
   "source": [
    "print('Cash by Client Shape: ', cash_by_client.shape)\n",
    "train = train.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "test = test.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del cash, cash_by_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2abca12f43e8d458385f66d734292e68e74a65e9"
   },
   "outputs": [],
   "source": [
    "train, test = remove_missing_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb6c3163f99dfd8b6703aa8f225fe26a81140973"
   },
   "source": [
    "## Feature Engineering: credit_card_balance (credit) dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60a34781e345601b855f7ca1abe8a6c900ab0e0a"
   },
   "outputs": [],
   "source": [
    "credit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\n",
    "credit = convert_types(credit, print_info = True)\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a762b8b6fa76fc1fc1158aebc707e91394bf4c8"
   },
   "outputs": [],
   "source": [
    "credit_by_client = aggregate_client(credit, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\n",
    "credit_by_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54bbd4683bd67d516091f277c9a9f3b10ce22cfc"
   },
   "outputs": [],
   "source": [
    "print('Credit by client shape: ', credit_by_client.shape)\n",
    "\n",
    "train = train.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "test = test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del credit, credit_by_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e79e32fd35e17def37a32ea9e3592c8fb5e8dee8"
   },
   "source": [
    "## Feature Engineering: installments_payment (installments) dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81b53cabea24c53dcc8fb252c237a13e99afc0b8"
   },
   "outputs": [],
   "source": [
    "installments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\n",
    "installments = convert_types(installments, print_info = True)\n",
    "installments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7909b31413f4591685fe43c76c7eca616b61e7d0"
   },
   "outputs": [],
   "source": [
    "installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\n",
    "installments_by_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "284ade20155196dc7c5232a1f6e2f017b53347e3"
   },
   "outputs": [],
   "source": [
    "print('Installments by client shape: ', installments_by_client.shape)\n",
    "\n",
    "train = train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "test = test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del installments, installments_by_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "646ff6d6bf2f2fcc4b24c7bbef5fd2639d3fc371"
   },
   "outputs": [],
   "source": [
    "train, test = remove_missing_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f774f5eb1b44cac651c855fe887d732a29c108f4"
   },
   "source": [
    "## Correlations\n",
    "Examine the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb869ea840b640f3f6f2e570e3c3f28da5950db8"
   },
   "outputs": [],
   "source": [
    "# Calculate all correlations in dataframe\n",
    "corrs = train.corr()\n",
    "\n",
    "corrs = corrs.sort_values('TARGET', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e42961e572ae35d8e8ff847c710b3f00c802187"
   },
   "outputs": [],
   "source": [
    "# Ten most positive correlations\n",
    "pd.DataFrame(corrs['TARGET'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13ce37bbb133c5330b99a63df6d3357d2bc1c971"
   },
   "outputs": [],
   "source": [
    "# Ten most negative correlations\n",
    "pd.DataFrame(corrs['TARGET'].dropna().tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f87df54ca5ca78c3dd2da2f31be621544220912"
   },
   "source": [
    "## Colinear Variables\n",
    "Calculate  the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. Look for any variables that have a greather than 0.8 correlation with other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62337d895e3d0b4784618ff0db0fb6e8d7d542d3"
   },
   "outputs": [],
   "source": [
    "# Set the threshold\n",
    "threshold = 0.8\n",
    "\n",
    "# Empty dictionary to hold correlated variables\n",
    "above_threshold_vars = {}\n",
    "\n",
    "# For each column, record the variables that are above the threshold\n",
    "for col in corrs:\n",
    "    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])\n",
    "    \n",
    "# Track columns to remove and columns already examined\n",
    "cols_to_remove = []\n",
    "cols_seen = []\n",
    "cols_to_remove_pair = []\n",
    "\n",
    "# Iterate through columns and correlated columns\n",
    "for key, value in above_threshold_vars.items():\n",
    "    # Keep track of columns already examined\n",
    "    cols_seen.append(key)\n",
    "    for x in value:\n",
    "        if x == key:\n",
    "            next\n",
    "        else:\n",
    "            # Only want to remove one in a pair\n",
    "            if x not in cols_seen:\n",
    "                cols_to_remove.append(x)\n",
    "                cols_to_remove_pair.append(key)\n",
    "            \n",
    "cols_to_remove = list(set(cols_to_remove))\n",
    "print('Number of columns to remove: ', len(cols_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3e0c713edb7bc6feafee41d464eac4ac1ae9eae"
   },
   "outputs": [],
   "source": [
    "# remove columns from training and testing sets\n",
    "train = train.drop(columns = cols_to_remove)\n",
    "test = test.drop(columns = cols_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddd701b7464d6f3e68cd0e308faf56e921f51100"
   },
   "source": [
    "# Final Train and Test Shape and Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20fa5217163d991d2048e72af36aa426d5375169"
   },
   "outputs": [],
   "source": [
    "print('Final Training Shape: ', train.shape)\n",
    "print('Final Testing Shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fc546b06318ec99e375c483f803d3d979f326e5"
   },
   "outputs": [],
   "source": [
    "print(f'Final training size: {return_size(train)}')\n",
    "print(f'Final testing size: {return_size(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fb53fee77a6654a62dcb5b564b8333eef18d8af"
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_engineered.csv', index = False, chunksize = 500)\n",
    "test.to_csv('test_engineered.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

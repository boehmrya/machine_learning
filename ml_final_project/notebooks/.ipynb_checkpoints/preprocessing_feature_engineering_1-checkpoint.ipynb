{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input/preprocessing-and-feature-engineering-one\"))\n\nplt.style.use('fivethirtyeight')\n\n# Memory management\nimport gc ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['__output__.json', '__results___files', 'custom.css', '__results__.html', 'test_corrs_removed.csv', 'train_corrs_removed.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f4f7a6ced7bacef6452d925430941434db5f99c9"
      },
      "cell_type": "markdown",
      "source": "# Import Data From Part One"
    },
    {
      "metadata": {
        "_uuid": "f286d677e1a561e65205d8285540dcc778432926"
      },
      "cell_type": "markdown",
      "source": "## Import Training Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c83de58d7e9323358bffa7b6a6436fc191760b4"
      },
      "cell_type": "code",
      "source": "# Training data\ntrain = pd.read_csv('../input/preprocessing-and-feature-engineering-one/train_corrs_removed.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training data shape:  (307511, 317)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "   SK_ID_CURR   ...    TARGET\n0      100002   ...         1\n1      100003   ...         0\n2      100004   ...         0\n3      100006   ...         0\n4      100007   ...         0\n\n[5 rows x 317 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>REGION_POPULATION_RELATIVE</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>DAYS_REGISTRATION</th>\n      <th>DAYS_ID_PUBLISH</th>\n      <th>OWN_CAR_AGE</th>\n      <th>FLAG_MOBIL</th>\n      <th>FLAG_EMP_PHONE</th>\n      <th>FLAG_WORK_PHONE</th>\n      <th>FLAG_CONT_MOBILE</th>\n      <th>FLAG_PHONE</th>\n      <th>FLAG_EMAIL</th>\n      <th>REGION_RATING_CLIENT</th>\n      <th>HOUR_APPR_PROCESS_START</th>\n      <th>REG_REGION_NOT_LIVE_REGION</th>\n      <th>REG_REGION_NOT_WORK_REGION</th>\n      <th>REG_CITY_NOT_LIVE_CITY</th>\n      <th>REG_CITY_NOT_WORK_CITY</th>\n      <th>EXT_SOURCE_1</th>\n      <th>EXT_SOURCE_2</th>\n      <th>EXT_SOURCE_3</th>\n      <th>APARTMENTS_AVG</th>\n      <th>BASEMENTAREA_AVG</th>\n      <th>YEARS_BEGINEXPLUATATION_AVG</th>\n      <th>YEARS_BUILD_AVG</th>\n      <th>COMMONAREA_AVG</th>\n      <th>ENTRANCES_AVG</th>\n      <th>FLOORSMAX_AVG</th>\n      <th>FLOORSMIN_AVG</th>\n      <th>LANDAREA_AVG</th>\n      <th>NONLIVINGAPARTMENTS_AVG</th>\n      <th>NONLIVINGAREA_AVG</th>\n      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n      <th>DAYS_LAST_PHONE_CHANGE</th>\n      <th>...</th>\n      <th>client_bureau_balance_STATUS_0_count_mean</th>\n      <th>client_bureau_balance_STATUS_0_count_max</th>\n      <th>client_bureau_balance_STATUS_0_count_min</th>\n      <th>client_bureau_balance_STATUS_0_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_0_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_0_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_1_count_mean</th>\n      <th>client_bureau_balance_STATUS_1_count_max</th>\n      <th>client_bureau_balance_STATUS_1_count_min</th>\n      <th>client_bureau_balance_STATUS_1_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_1_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_1_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_2_count_mean</th>\n      <th>client_bureau_balance_STATUS_2_count_max</th>\n      <th>client_bureau_balance_STATUS_2_count_min</th>\n      <th>client_bureau_balance_STATUS_2_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_2_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_3_count_mean</th>\n      <th>client_bureau_balance_STATUS_3_count_max</th>\n      <th>client_bureau_balance_STATUS_3_count_min</th>\n      <th>client_bureau_balance_STATUS_3_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_3_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_4_count_mean</th>\n      <th>client_bureau_balance_STATUS_4_count_max</th>\n      <th>client_bureau_balance_STATUS_4_count_min</th>\n      <th>client_bureau_balance_STATUS_4_count_sum</th>\n      <th>client_bureau_balance_STATUS_4_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_5_count_mean</th>\n      <th>client_bureau_balance_STATUS_5_count_max</th>\n      <th>client_bureau_balance_STATUS_5_count_min</th>\n      <th>client_bureau_balance_STATUS_C_count_mean</th>\n      <th>client_bureau_balance_STATUS_C_count_min</th>\n      <th>client_bureau_balance_STATUS_C_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_X_count_mean</th>\n      <th>client_bureau_balance_STATUS_X_count_max</th>\n      <th>client_bureau_balance_STATUS_X_count_min</th>\n      <th>client_bureau_balance_STATUS_X_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_X_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_X_count_norm_min</th>\n      <th>TARGET</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100002</td>\n      <td>0</td>\n      <td>202500.0</td>\n      <td>406597.5</td>\n      <td>24700.5</td>\n      <td>0.018801</td>\n      <td>9461</td>\n      <td>-637</td>\n      <td>-3648.0</td>\n      <td>-2120</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.083037</td>\n      <td>0.262949</td>\n      <td>0.139376</td>\n      <td>0.0247</td>\n      <td>0.0369</td>\n      <td>0.9722</td>\n      <td>0.6192</td>\n      <td>0.0143</td>\n      <td>0.0690</td>\n      <td>0.0833</td>\n      <td>0.1250</td>\n      <td>0.0369</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>-1134.0</td>\n      <td>...</td>\n      <td>5.625</td>\n      <td>18.0</td>\n      <td>2.0</td>\n      <td>0.40696</td>\n      <td>0.818182</td>\n      <td>0.1875</td>\n      <td>3.375</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>0.255682</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.875</td>\n      <td>0.0</td>\n      <td>0.8125</td>\n      <td>1.875</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.161932</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100003</td>\n      <td>0</td>\n      <td>270000.0</td>\n      <td>1293502.5</td>\n      <td>35698.5</td>\n      <td>0.003541</td>\n      <td>16765</td>\n      <td>-1188</td>\n      <td>-1186.0</td>\n      <td>-291</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.311267</td>\n      <td>0.622246</td>\n      <td>NaN</td>\n      <td>0.0959</td>\n      <td>0.0529</td>\n      <td>0.9851</td>\n      <td>0.7960</td>\n      <td>0.0605</td>\n      <td>0.0345</td>\n      <td>0.2917</td>\n      <td>0.3333</td>\n      <td>0.0130</td>\n      <td>0.0039</td>\n      <td>0.0098</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-828.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100004</td>\n      <td>0</td>\n      <td>67500.0</td>\n      <td>135000.0</td>\n      <td>6750.0</td>\n      <td>0.010032</td>\n      <td>19046</td>\n      <td>-225</td>\n      <td>-4260.0</td>\n      <td>-2531</td>\n      <td>26.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.555912</td>\n      <td>0.729567</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-815.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100006</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>312682.5</td>\n      <td>29686.5</td>\n      <td>0.008019</td>\n      <td>19005</td>\n      <td>-3039</td>\n      <td>-9833.0</td>\n      <td>-2437</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.650442</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>-617.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100007</td>\n      <td>0</td>\n      <td>121500.0</td>\n      <td>513000.0</td>\n      <td>21865.5</td>\n      <td>0.028663</td>\n      <td>19932</td>\n      <td>-3038</td>\n      <td>-4311.0</td>\n      <td>-3458</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0.322738</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1106.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "12cedcbb7c51971aa8a5e63d76fe660df0ae2594"
      },
      "cell_type": "markdown",
      "source": "## Import Testing Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ea002847fcbe63c0a3df380addc58a962081ca8"
      },
      "cell_type": "code",
      "source": "# Testing data\ntest = pd.read_csv('../input/preprocessing-and-feature-engineering-one/test_corrs_removed.csv')\nprint('Training data shape: ', test.shape)\ntest.head()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training data shape:  (48744, 316)\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "   SK_ID_CURR                      ...                        client_bureau_balance_STATUS_X_count_norm_min\n0      100001                      ...                                                                  0.0\n1      100005                      ...                                                                  0.0\n2      100013                      ...                                                                  0.0\n3      100028                      ...                                                                  0.0\n4      100038                      ...                                                                  NaN\n\n[5 rows x 316 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>REGION_POPULATION_RELATIVE</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>DAYS_REGISTRATION</th>\n      <th>DAYS_ID_PUBLISH</th>\n      <th>OWN_CAR_AGE</th>\n      <th>FLAG_MOBIL</th>\n      <th>FLAG_EMP_PHONE</th>\n      <th>FLAG_WORK_PHONE</th>\n      <th>FLAG_CONT_MOBILE</th>\n      <th>FLAG_PHONE</th>\n      <th>FLAG_EMAIL</th>\n      <th>REGION_RATING_CLIENT</th>\n      <th>HOUR_APPR_PROCESS_START</th>\n      <th>REG_REGION_NOT_LIVE_REGION</th>\n      <th>REG_REGION_NOT_WORK_REGION</th>\n      <th>REG_CITY_NOT_LIVE_CITY</th>\n      <th>REG_CITY_NOT_WORK_CITY</th>\n      <th>EXT_SOURCE_1</th>\n      <th>EXT_SOURCE_2</th>\n      <th>EXT_SOURCE_3</th>\n      <th>APARTMENTS_AVG</th>\n      <th>BASEMENTAREA_AVG</th>\n      <th>YEARS_BEGINEXPLUATATION_AVG</th>\n      <th>YEARS_BUILD_AVG</th>\n      <th>COMMONAREA_AVG</th>\n      <th>ENTRANCES_AVG</th>\n      <th>FLOORSMAX_AVG</th>\n      <th>FLOORSMIN_AVG</th>\n      <th>LANDAREA_AVG</th>\n      <th>NONLIVINGAPARTMENTS_AVG</th>\n      <th>NONLIVINGAREA_AVG</th>\n      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n      <th>DAYS_LAST_PHONE_CHANGE</th>\n      <th>...</th>\n      <th>client_bureau_balance_MONTHS_BALANCE_max_max</th>\n      <th>client_bureau_balance_STATUS_0_count_mean</th>\n      <th>client_bureau_balance_STATUS_0_count_max</th>\n      <th>client_bureau_balance_STATUS_0_count_min</th>\n      <th>client_bureau_balance_STATUS_0_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_0_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_0_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_1_count_mean</th>\n      <th>client_bureau_balance_STATUS_1_count_max</th>\n      <th>client_bureau_balance_STATUS_1_count_min</th>\n      <th>client_bureau_balance_STATUS_1_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_1_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_1_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_2_count_mean</th>\n      <th>client_bureau_balance_STATUS_2_count_max</th>\n      <th>client_bureau_balance_STATUS_2_count_min</th>\n      <th>client_bureau_balance_STATUS_2_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_2_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_3_count_mean</th>\n      <th>client_bureau_balance_STATUS_3_count_max</th>\n      <th>client_bureau_balance_STATUS_3_count_min</th>\n      <th>client_bureau_balance_STATUS_3_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_3_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_4_count_mean</th>\n      <th>client_bureau_balance_STATUS_4_count_max</th>\n      <th>client_bureau_balance_STATUS_4_count_min</th>\n      <th>client_bureau_balance_STATUS_4_count_sum</th>\n      <th>client_bureau_balance_STATUS_4_count_norm_min</th>\n      <th>client_bureau_balance_STATUS_5_count_mean</th>\n      <th>client_bureau_balance_STATUS_5_count_max</th>\n      <th>client_bureau_balance_STATUS_5_count_min</th>\n      <th>client_bureau_balance_STATUS_C_count_mean</th>\n      <th>client_bureau_balance_STATUS_C_count_min</th>\n      <th>client_bureau_balance_STATUS_C_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_X_count_mean</th>\n      <th>client_bureau_balance_STATUS_X_count_max</th>\n      <th>client_bureau_balance_STATUS_X_count_min</th>\n      <th>client_bureau_balance_STATUS_X_count_norm_mean</th>\n      <th>client_bureau_balance_STATUS_X_count_norm_max</th>\n      <th>client_bureau_balance_STATUS_X_count_norm_min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100001</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>568800.0</td>\n      <td>20560.5</td>\n      <td>0.018850</td>\n      <td>-19241</td>\n      <td>-2329.0</td>\n      <td>-5170.0</td>\n      <td>-812</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.752614</td>\n      <td>0.789654</td>\n      <td>0.159520</td>\n      <td>0.0660</td>\n      <td>0.0590</td>\n      <td>0.9732</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.1379</td>\n      <td>0.125</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1740.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4.428571</td>\n      <td>12.0</td>\n      <td>1.0</td>\n      <td>0.336651</td>\n      <td>1.000000</td>\n      <td>0.019231</td>\n      <td>0.142857</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.007519</td>\n      <td>0.052632</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.714286</td>\n      <td>0.0</td>\n      <td>0.966667</td>\n      <td>4.285714</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.214590</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100005</td>\n      <td>0</td>\n      <td>99000.0</td>\n      <td>222768.0</td>\n      <td>17370.0</td>\n      <td>0.035792</td>\n      <td>-18064</td>\n      <td>-4469.0</td>\n      <td>-9118.0</td>\n      <td>-1623</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.564990</td>\n      <td>0.291656</td>\n      <td>0.432962</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4.666667</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>0.735043</td>\n      <td>1.000000</td>\n      <td>0.538462</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.666667</td>\n      <td>0.0</td>\n      <td>0.384615</td>\n      <td>0.666667</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.136752</td>\n      <td>0.333333</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100013</td>\n      <td>0</td>\n      <td>202500.0</td>\n      <td>663264.0</td>\n      <td>69777.0</td>\n      <td>0.019101</td>\n      <td>-20038</td>\n      <td>-4458.0</td>\n      <td>-2175.0</td>\n      <td>-3503</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.699787</td>\n      <td>0.610991</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-856.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>19.750000</td>\n      <td>34.0</td>\n      <td>0.0</td>\n      <td>0.320718</td>\n      <td>0.618182</td>\n      <td>0.000000</td>\n      <td>1.750000</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.027701</td>\n      <td>0.045455</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>25.750000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>10.250000</td>\n      <td>40.0</td>\n      <td>0.0</td>\n      <td>0.254545</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100028</td>\n      <td>2</td>\n      <td>315000.0</td>\n      <td>1575000.0</td>\n      <td>49018.5</td>\n      <td>0.026392</td>\n      <td>-13976</td>\n      <td>-1866.0</td>\n      <td>-2000.0</td>\n      <td>-4208</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.525734</td>\n      <td>0.509677</td>\n      <td>0.612704</td>\n      <td>0.3052</td>\n      <td>0.1974</td>\n      <td>0.9970</td>\n      <td>0.9592</td>\n      <td>0.1165</td>\n      <td>0.2759</td>\n      <td>0.375</td>\n      <td>0.0417</td>\n      <td>0.2042</td>\n      <td>0.0386</td>\n      <td>0.08</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1805.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>13.666667</td>\n      <td>34.0</td>\n      <td>0.0</td>\n      <td>0.377321</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>21.916667</td>\n      <td>0.0</td>\n      <td>0.885714</td>\n      <td>11.083333</td>\n      <td>60.0</td>\n      <td>0.0</td>\n      <td>0.260434</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100038</td>\n      <td>1</td>\n      <td>180000.0</td>\n      <td>625500.0</td>\n      <td>32067.0</td>\n      <td>0.010032</td>\n      <td>-13040</td>\n      <td>-2191.0</td>\n      <td>-4000.0</td>\n      <td>-4262</td>\n      <td>16.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.202145</td>\n      <td>0.425687</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-821.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "# Useful Functions"
    },
    {
      "metadata": {
        "_uuid": "cc1266bc40ddc394dbedd967c01939e00595c20a"
      },
      "cell_type": "markdown",
      "source": "## Function to Aggregate Numeric Data\nThis groups data by the group_var and calculates mean, max, min, and sum. It will only be applied to numeric data by default in pandas."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b0c07c602ca87c3724aa6e8cabf8916a86e03d5"
      },
      "cell_type": "code",
      "source": "def agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "29d3b477e08d1c67b78a1eca83d4db5162576e0a"
      },
      "cell_type": "markdown",
      "source": "## Function to Calculate Categorical Count\nThis calculates the occurrences (counts) of each category in a categorical variable for each client. It also calculates the normed count, which is the count for a category divided by the total counts for all categories in a categorical variable."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3dc87e670deb50c5b8a906391abaae9a9dc31cc8"
      },
      "cell_type": "code",
      "source": "def agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('category'))\n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "218ba8c1634081f158cc39b48124d07ce5cecfa0"
      },
      "cell_type": "markdown",
      "source": "## Function to Aggregate Stats at the Client Level"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2acf90ab5b22eb2ae39c9c2e1671d9a3eb04eacf"
      },
      "cell_type": "code",
      "source": "def aggregate_client(df, group_vars, df_names):\n    \"\"\"Aggregate a dataframe with data at the loan level \n    at the client level\n    \n    Args:\n        df (dataframe): data at the loan level\n        group_vars (list of two strings): grouping variables for the loan \n        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n        names (list of two strings): names to call the resulting columns\n        (example ['cash', 'client'])\n        \n    Returns:\n        df_client (dataframe): aggregated numeric stats at the client level. \n        Each client will have a single row with all the numeric data aggregated\n    \"\"\"\n    \n    # Aggregate the numeric columns\n    df_agg = agg_numeric(df, parent_var = group_vars[0], df_name = df_names[0])\n    \n    # If there are categorical variables\n    if any(df.dtypes == 'category'):\n    \n        # Count the categorical columns\n        df_counts = agg_categorical(df, parent_var = group_vars[0], df_name = df_names[0])\n\n        # Merge the numeric and categorical\n        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n\n        gc.enable()\n        del df_agg, df_counts\n        gc.collect()\n\n        # Merge to get the client id in dataframe\n        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n\n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n\n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n\n        \n    # No categorical variables\n    else:\n        # Merge to get the client id in dataframe\n        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n        \n        gc.enable()\n        del df_agg\n        gc.collect()\n        \n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n        \n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n        \n    # Memory management\n    gc.enable()\n    del df, df_by_loan\n    gc.collect()\n\n    return df_by_client",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "898af47d49985f94e2ca4340a1b8ee21130e26ac"
      },
      "cell_type": "markdown",
      "source": "## Function for KDE Plots of Variable\nThis plots the distribution of variable colored by the value of TARGET (either 1 for did not repay the loan or 0 for did repay the loan). We can use this function to visually examine any new variables we create. This also calculates the correlation cofficient of the variable with the target which can be used as an approximation of whether or not the created variable will be useful."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88c7136315296bbe60c07c0417635a8c77c19868"
      },
      "cell_type": "code",
      "source": "# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b587a2ed1af10450cb90fc31c916ddca7762fa8a"
      },
      "cell_type": "markdown",
      "source": "## Function to Convert Data Types\nThis will help reduce memory usage by using more efficient types for the variables. For example category is often a better type than object (unless the number of unique categories is close to the number of rows in the dataframe)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb4f8cf6cfa9e0cba88718832993d9f4b9454e0c"
      },
      "cell_type": "code",
      "source": "import sys\n\ndef return_size(df):\n    \"\"\"Return size of dataframe in gigabytes\"\"\"\n    return round(sys.getsizeof(df) / 1e9, 2)\n\ndef convert_types(df, print_info = False):\n    \n    original_memory = df.memory_usage().sum()\n    \n    # Iterate through each column\n    for c in df:\n        \n        # Convert ids and booleans to integers\n        if ('SK_ID' in c):\n            df[c] = df[c].fillna(0).astype(np.int32)\n            \n        # Convert objects to category\n        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n            df[c] = df[c].astype('category')\n        \n        # Booleans mapped to integers\n        elif list(df[c].unique()) == [1, 0]:\n            df[c] = df[c].astype(bool)\n        \n        # Float64 to float32\n        elif df[c].dtype == float:\n            df[c] = df[c].astype(np.float32)\n            \n        # Int64 to int32\n        elif df[c].dtype == int:\n            df[c] = df[c].astype(np.int32)\n        \n    new_memory = df.memory_usage().sum()\n    \n    if print_info:\n        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n        \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3749467140b877d9d2d2b45f6c921782faf922b6"
      },
      "cell_type": "markdown",
      "source": "## Function to Calculate Missing Values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb640a9b6fe20a5f9f6a6d0f5d724baffb3d1239"
      },
      "cell_type": "code",
      "source": "# Function to calculate missing values by column# Funct \ndef missing_values_table(df, print_info = False):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        if print_info:\n            # Print some summary information\n            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n                  \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1bbbf70e67bdd2e632ec9740f094327e1472fc5"
      },
      "cell_type": "code",
      "source": "def remove_missing_columns(train, test, threshold = 80):\n    # Calculate missing stats for train and test (remember to calculate a percent!)\n    train_miss = pd.DataFrame(train.isnull().sum())\n    train_miss['percent'] = 100 * train_miss[0] / len(train)\n    \n    test_miss = pd.DataFrame(test.isnull().sum())\n    test_miss['percent'] = 100 * test_miss[0] / len(test)\n    \n    # list of missing columns for train and test\n    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n    \n    # Combine the two lists together\n    missing_columns = list(set(missing_train_columns + missing_test_columns))\n    \n    # Print information\n    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n    \n    # Drop the missing columns and return\n    train = train.drop(columns = missing_columns)\n    test = test.drop(columns = missing_columns)\n    \n    return train, test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88c933feb1c392790ce82aca2da7939f639976aa"
      },
      "cell_type": "markdown",
      "source": "# Feature Engineering: one dataframe at a time for final four dataframes\nDataframes: previous_application, POS_CASH_BALANCE (cash), credit_card_balance (credit), installments_payment (installments)"
    },
    {
      "metadata": {
        "_uuid": "4df3f640a65785bab84fa3c0f0f493da88a6361a"
      },
      "cell_type": "markdown",
      "source": "## Feature Engineering: previous_application dataframe\nprevious applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c518228271c11319f01246a1ec50e657485a77b"
      },
      "cell_type": "code",
      "source": "previous = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\nprevious = convert_types(previous, print_info=True)\nprevious.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7376c3d5f21119558374ec5f975eb31ba093ea8"
      },
      "cell_type": "code",
      "source": "# Calculate aggregate statistics for each numeric column\nprevious_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\nprint('Previous aggregation shape: ', previous_agg.shape)\nprevious_agg.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d932d84ca5473a8deabc25b3fee32dc92675eba5"
      },
      "cell_type": "code",
      "source": "# Calculate value counts for each categorical column\nprevious_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\nprint('Previous counts shape: ', previous_counts.shape)\nprevious_counts.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c0dbbbda34ca4db6bb835b47e24a0c95bcff8bd"
      },
      "cell_type": "code",
      "source": "## Merge into training and testing data\ntrain = convert_types(train)\ntest = convert_types(test)\n\n# Merge in the previous information\ntrain = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\ntrain = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n\ntest = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\ntest = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Remove variables to free memory\ngc.enable()\ndel previous, previous_agg, previous_counts\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d867ac6845e8d69cfd88510a121dfade9e7b2b9"
      },
      "cell_type": "code",
      "source": "train, test = remove_missing_columns(train, test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4de12a01afd5ee13fe5585fa8772cf070604ddb0"
      },
      "cell_type": "markdown",
      "source": "## Feature Engineering: POS_CASH_BALANCE (cash) dataframe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54640cceaedbcd9f2bf337c4a9eabd6ab805c0f9"
      },
      "cell_type": "code",
      "source": "cash = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\ncash = convert_types(cash, print_info=True)\ncash.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c2f41fef20b4127b52d9c9cd05049f1896ec9de"
      },
      "cell_type": "code",
      "source": "cash_by_client = aggregate_client(cash, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\ncash_by_client.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd619ff59d7a07998db17cb9c1be9a67755bb8f8"
      },
      "cell_type": "code",
      "source": "print('Cash by Client Shape: ', cash_by_client.shape)\ntrain = train.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel cash, cash_by_client\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2abca12f43e8d458385f66d734292e68e74a65e9"
      },
      "cell_type": "code",
      "source": "train, test = remove_missing_columns(train, test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb6c3163f99dfd8b6703aa8f225fe26a81140973"
      },
      "cell_type": "markdown",
      "source": "## Feature Engineering: credit_card_balance (credit) dataframe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60a34781e345601b855f7ca1abe8a6c900ab0e0a"
      },
      "cell_type": "code",
      "source": "credit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\ncredit = convert_types(credit, print_info = True)\ncredit.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a762b8b6fa76fc1fc1158aebc707e91394bf4c8"
      },
      "cell_type": "code",
      "source": "credit_by_client = aggregate_client(credit, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\ncredit_by_client.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54bbd4683bd67d516091f277c9a9f3b10ce22cfc"
      },
      "cell_type": "code",
      "source": "print('Credit by client shape: ', credit_by_client.shape)\n\ntrain = train.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel credit, credit_by_client\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e79e32fd35e17def37a32ea9e3592c8fb5e8dee8"
      },
      "cell_type": "markdown",
      "source": "## Feature Engineering: installments_payment (installments) dataframe"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81b53cabea24c53dcc8fb252c237a13e99afc0b8"
      },
      "cell_type": "code",
      "source": "installments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\ninstallments = convert_types(installments, print_info = True)\ninstallments.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7909b31413f4591685fe43c76c7eca616b61e7d0"
      },
      "cell_type": "code",
      "source": "installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\ninstallments_by_client.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "284ade20155196dc7c5232a1f6e2f017b53347e3"
      },
      "cell_type": "code",
      "source": "print('Installments by client shape: ', installments_by_client.shape)\n\ntrain = train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel installments, installments_by_client\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "646ff6d6bf2f2fcc4b24c7bbef5fd2639d3fc371"
      },
      "cell_type": "code",
      "source": "train, test = remove_missing_columns(train, test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f774f5eb1b44cac651c855fe887d732a29c108f4"
      },
      "cell_type": "markdown",
      "source": "## Correlations\nExamine the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb869ea840b640f3f6f2e570e3c3f28da5950db8"
      },
      "cell_type": "code",
      "source": "# Calculate all correlations in dataframe\ncorrs = train.corr()\n\ncorrs = corrs.sort_values('TARGET', ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e42961e572ae35d8e8ff847c710b3f00c802187"
      },
      "cell_type": "code",
      "source": "# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13ce37bbb133c5330b99a63df6d3357d2bc1c971"
      },
      "cell_type": "code",
      "source": "# Ten most negative correlations\npd.DataFrame(corrs['TARGET'].dropna().tail(10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5f87df54ca5ca78c3dd2da2f31be621544220912"
      },
      "cell_type": "markdown",
      "source": "## Colinear Variables\nCalculate  the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. Look for any variables that have a greather than 0.8 correlation with other variables."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62337d895e3d0b4784618ff0db0fb6e8d7d542d3"
      },
      "cell_type": "code",
      "source": "# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])\n    \n# Track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3e0c713edb7bc6feafee41d464eac4ac1ae9eae"
      },
      "cell_type": "code",
      "source": "# remove columns from training and testing sets\ntrain = train.drop(columns = cols_to_remove)\ntest = test.drop(columns = cols_to_remove)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ddd701b7464d6f3e68cd0e308faf56e921f51100"
      },
      "cell_type": "markdown",
      "source": "# Final Train and Test Shape and Size "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20fa5217163d991d2048e72af36aa426d5375169"
      },
      "cell_type": "code",
      "source": "print('Final Training Shape: ', train.shape)\nprint('Final Testing Shape: ', test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4fc546b06318ec99e375c483f803d3d979f326e5"
      },
      "cell_type": "code",
      "source": "print(f'Final training size: {return_size(train)}')\nprint(f'Final testing size: {return_size(test)}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fb53fee77a6654a62dcb5b564b8333eef18d8af"
      },
      "cell_type": "code",
      "source": "train.to_csv('train_engineered.csv', index = False, chunksize = 500)\ntest.to_csv('test_engineered.csv', index = False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}